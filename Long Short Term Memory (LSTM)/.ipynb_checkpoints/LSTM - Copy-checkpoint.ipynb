{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2a322",
   "metadata": {},
   "source": [
    "# 1. Siapkan fungsi daret waktu yang akan digunakan untuk perhitungan nantinya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8595685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - look_back - 1):\n",
    "        a = dataset[i:(i + look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31803727",
   "metadata": {},
   "source": [
    "# 2. Gunakan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839df414",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = read_csv('airline-passengers.csv', sep=\",\")\n",
    "df = df.drop(['Month'], axis=1)\n",
    "display(df)\n",
    "dataset = df.astype('float32')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee44a75",
   "metadata": {},
   "source": [
    "# 3. Lakukan normalisasi min-max pada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8447b",
   "metadata": {},
   "source": [
    "# 4. Pisahkan data yang digunakan untuk train dan test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c09d8",
   "metadata": {},
   "source": [
    "##### mengambil data 67% untuk digunakan sebagai train_size dan sisanya digunakan sebagai tesgt_size\n",
    "##### \"train = dataset[0:train_size, :]\" = berarti bahwa mengambil data pada dataset yang dimulai dengan index ke 0 hingga sepanjang index hasil perhitungan train_test yang telah dihitung.\n",
    "##### \"test = dataset[train_size:len(dataset), :]\" = berarti bahwa mengambil data yang dimulai dari index hasil perhitungan train_size hingga akhir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfbd453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "print(f\"\"\"Berikut adalah hasil dari train dan test\n",
    "{train_size}, {test_size}\"\"\")\n",
    "\n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e29c1",
   "metadata": {},
   "source": [
    "# 5. Buat bentuk data menjadi siap untuk LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dbdfc",
   "metadata": {},
   "source": [
    "##### look_back = berapa banyak data sebelumnya yang akan dilihat untuk menjadi dasar dalam pengambilan keputusan\n",
    "##### gunakan fungsi create_dataset sebelumnya untuk melakkan proses train dan test\n",
    "- trainX, trainY = dianggap sebagai dataX, dataY pada fungsi create_dataset\n",
    "- testX, testY = dianggap sebagai dataX, dataY pada fungsi create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b21a4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "print(f\"\"\"Berikut adalah hasil dari trainX:\n",
    "{trainX}\n",
    "Berikut adalah hasil dari trainY:\n",
    "{trainY}\n",
    "Berikut adalah hasil dari testX:\n",
    "{testX}\n",
    "Berikut adalah hasil dari testY:\n",
    "{testY}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee19781",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0943fc",
   "metadata": {},
   "source": [
    "# 6. Desain algoritma LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8fa13",
   "metadata": {},
   "source": [
    "### model = Sequential()\n",
    "Sequential(): Bayangkan model ini seperti tumpukan lego, di mana kita menambah lapisan demi lapisan. Sequential adalah alat yang membantu kita membuat tumpukan model ini dengan mudah.\n",
    "\n",
    "### 1. model.add(LSTM(50, return_sequences=True, input_shape=(1, look_back)))\n",
    "\n",
    "##### model.add(LSTM(50, ....))\n",
    "- Menambahkan lapisan LSTM pertama dengan 50 unit atau neuron.\n",
    "- Setiap unit atau neuron LSTM memiliki cell state dan hidden state yang dapat mempertahankan informasi jangka panjang, sehingga membantu mempelajari pola di data deret waktu.\n",
    "\n",
    "LSTM(50): LSTM adalah bagian dari model yang sangat pintar dalam mengingat pola-pola dari masa lalu. Misalnya, model ini akan melihat jumlah penumpang selama 12 bulan terakhir untuk mempelajari apakah ada pola yang berulang. Jadi, kalau bulan-bulan tertentu selalu ramai, dia bisa “mengingatnya” dan memprediksi bulan berikutnya dengan lebih baik.\n",
    "\n",
    "##### input_shape=(1, look_back)\n",
    "- bentuk asli adalah (time_steps, features)\n",
    "- time_steps = 1: Artinya model membaca satu urutan titik data dalam satu langkah waktu (ini sering digunakan dalam data deret waktu yang diubah ke dalam bentuk [samples, time_steps, features]).\n",
    "- features = look_back: Jumlah features adalah look_back, yaitu jumlah titik data sebelumnya yang ingin diamati oleh model untuk memprediksi nilai berikutnya.\n",
    "\n",
    "input_shape=(1, look_back): Ini memberitahu model bahwa setiap kali melihat data, kita ingin dia memperhatikan 12 bulan sebelumnya, satu per satu, agar ia bisa mengenali pola.\n",
    "\n",
    "##### return_sequences=True\n",
    "- Memastikan bahwa lapisan LSTM pertama mengeluarkan seluruh rangkaian (sequence) keluaran, bukan hanya keluaran terakhir.\n",
    "- Ketika return_sequences=True, output dari setiap langkah waktu di LSTM pertama akan diteruskan ke lapisan berikutnya. Ini memungkinkan lapisan berikutnya menerima seluruh rangkaian keluaran untuk menangkap pola yang lebih kompleks.\n",
    "\n",
    "return_sequences=True: Ini seperti menyuruh model untuk terus mengalirkan informasi dari setiap bulan sebelumnya ke bulan berikutnya. Jadi, model akan mengingat informasi dari setiap bulan, bukan hanya mengambil hasil akhir saja.\n",
    "\n",
    "### 2. model.add(LSTM(50))\n",
    "- Menambahkan lapisan LSTM kedua dengan 50 unit (neuron).\n",
    "- Pada lapisan ini, return_sequences=False secara default, artinya lapisan ini hanya mengeluarkan keluaran terakhir dari urutan waktu, bukan seluruh urutan.\n",
    "- Hal ini cocok untuk menyimpulkan output akhir yang akan digunakan untuk prediksi, karena output akhir dari LSTM ini akan diteruskan ke lapisan Dense untuk memprediksi nilai target.\n",
    "- Lapisan ini menangkap pola yang lebih kompleks dan mempelajari urutan dari keluaran lapisan pertama, yang mencerminkan pola jangka panjang dan jangka pendek.\n",
    "\n",
    "Lapisan ini juga pintar dalam mengingat pola, tetapi di sini kita hanya ingin hasil akhirnya saja. Lapisan ini akan mengambil semua informasi dari bulan-bulan sebelumnya dan membuat satu kesimpulan besar. Dengan begitu, model ini jadi lebih siap untuk membuat prediksi bulan berikutnya.\n",
    "\n",
    "### 3. model.add(Dense(1))\n",
    "- Menambahkan lapisan Dense (lapisan fully connected) dengan 1 neuron.\n",
    "- Fungsi lapisan ini adalah menghasilkan output akhir (dalam hal ini, memprediksi nilai penumpang pada bulan berikutnya).\n",
    "- Karena masalah ini adalah regresi (prediksi nilai kontinu), neuron ini tidak memiliki fungsi aktivasi tambahan. Neuron ini menerima nilai keluaran dari LSTM sebelumnya dan menghasilkan prediksi.\n",
    "\n",
    "Dense(1): Lapisan ini seperti lembar jawaban. Setelah model memproses semua data, ia akan menghasilkan satu angka di sini, yaitu tebakan jumlah penumpang di bulan berikutnya. Ibaratnya, ini adalah jawaban akhir model setelah melihat semua pola.\n",
    "\n",
    "### 4. model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "##### loss='mean_squared_error'\n",
    "- Fungsi kehilangan yang digunakan adalah Mean Squared Error (MSE), yang sering digunakan dalam masalah regresi. MSE mengukur rata-rata dari kuadrat kesalahan antara prediksi dan nilai sebenarnya. Semakin rendah nilai MSE, semakin akurat prediksi model.\n",
    "\n",
    "loss='mean_squared_error': Ini adalah cara menghitung kesalahan model, atau seberapa jauh tebakan model dari jawaban yang benar. Model akan terus mencoba menurunkan kesalahan ini, jadi semakin lama semakin akurat.\n",
    "\n",
    "##### optimizer='adam'\n",
    "- Optimizer yang digunakan adalah Adam (Adaptive Moment Estimation), yang merupakan kombinasi dari momentum dan RMSprop. Adam adalah pilihan umum karena biasanya memberikan konvergensi yang lebih cepat dan stabil dalam banyak kasus. Adam secara otomatis menyesuaikan laju belajar selama pelatihan.\n",
    "\n",
    "optimizer='adam': Optimizer ini seperti alat bantu untuk mempercepat model belajar. Jadi, \"Adam\" membantu model untuk belajar dengan cepat dan efisien.\n",
    "\n",
    "### 5. early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "##### monitor='val_loss'\n",
    "- Mengarahkan callback untuk memantau metrik val_loss (kerugian pada data validasi) untuk menentukan kapan pelatihan harus dihentikan. Ketika val_loss berhenti menurun, berarti model sudah mencapai batas kinerjanya pada data baru.\n",
    "\n",
    "monitor='val_loss': Model ini tidak hanya dilatih, tapi juga diuji pada data baru yang belum pernah dilihatnya. Ini untuk memastikan model tidak “menghafal” saja, tapi benar-benar memahami pola.\n",
    "\n",
    "##### patience=10\n",
    "- Menentukan berapa banyak epoch tambahan yang akan dijalankan setelah val_loss berhenti membaik. Jika setelah 10 epoch tidak ada peningkatan, pelatihan akan dihentikan.\n",
    "\n",
    "patience=10: Kalau model mencoba memperbaiki diri 10 kali tapi tetap tidak ada peningkatan, maka pelatihannya akan dihentikan. Ini seperti kita bilang, “Kalau sudah mentok, ya sudah berhenti saja, jangan terlalu lama belajar.” Ini mencegah model dari “overfitting,” yaitu kalau dia terlalu “menghafal” data, bisa jadi kurang bagus saat melihat data baru.\n",
    "\n",
    "##### restore_best_weights=True\n",
    "- Menginstruksikan model untuk memuat bobot terbaik yang dicapai selama pelatihan, yaitu bobot yang memiliki nilai val_loss terendah, sehingga model disimpan pada kondisi optimalnya.\n",
    "\n",
    "restore_best_weights=True: Model akan menyimpan hasil terbaiknya selama pelatihan. Jadi, ketika berhenti, model kita ada di kondisi terbaiknya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d04e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(1, look_back)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0,2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d3e60",
   "metadata": {},
   "source": [
    "### model.fit()\n",
    "- Metode .fit() adalah fungsi yang digunakan untuk melatih (training) model deep learning pada data yang telah disiapkan.\n",
    "- Di sini, .fit() mengambil data input trainX dan target trainY, serta beberapa parameter penting lainnya yang mengatur proses pelatihan model.\n",
    "\n",
    "model.fit() itu seperti memberitahu robot, \"Oke, sekarang waktunya belajar.\" Kita memberi robot data, cara belajar, dan batasan-batasan lain agar dia tidak belajar terlalu lama atau menjadi “kelelahan” dan melakukan kesalahan. \n",
    "\n",
    "### 1. Parameter epochs=100\n",
    "- Parameter epochs menentukan jumlah epoch atau putaran pelatihan model. Satu epoch berarti model akan melihat seluruh data pelatihan sekali dan menyesuaikan bobot untuk meminimalkan kesalahan.\n",
    "- Dalam kasus ini, model akan melihat data pelatihan sebanyak 100 kali, dengan harapan bahwa model dapat menemukan pola yang lebih baik di antara variabel input dan target untuk membuat prediksi yang lebih akurat.\n",
    "\n",
    "epochs=100 itu artinya robot akan melihat semua data yang kita berikan sebanyak 100 kali. Bayangkan belajar buku pelajaran 100 kali untuk benar-benar memahaminya—tujuannya agar robot semakin pintar mengenali polanya.\n",
    "\n",
    "### 2. Parameter batch_size=32\n",
    "- Batch size adalah jumlah sampel data yang diproses sebelum pembaruan parameter model dilakukan.\n",
    "- Dengan batch_size=32, berarti model akan membagi data pelatihan ke dalam kelompok-kelompok kecil yang berisi 32 sampel setiap kali, lalu memperbarui parameter setelah melihat setiap kelompok.\n",
    "- Proses ini mengurangi beban komputasi karena model tidak memproses semua data sekaligus, yang lebih efisien dan membantu menghindari risiko “kebingungan” dari keseluruhan dataset saat sekali melihat.\n",
    "\n",
    "batch_size=32 artinya robot belajar dalam kelompok kecil. Alih-alih belajar seluruh buku sekaligus, robot belajar 32 halaman setiap kali, sehingga tidak terlalu bingung dan bisa belajar dengan lebih cepat.\n",
    "\n",
    "### 3. Parameter validation_split=0.2\n",
    "- validation_split memisahkan sebagian data pelatihan menjadi data validasi untuk menilai kinerja model pada data yang tidak terlihat saat pelatihan.\n",
    "- Dengan validation_split=0.2, 20% dari data pelatihan akan digunakan sebagai data validasi, sementara sisanya 80% digunakan untuk pelatihan.\n",
    "- Data validasi ini penting untuk mendeteksi overfitting, yaitu saat model mulai “terlalu menyesuaikan” data pelatihan hingga tidak dapat bekerja baik pada data yang baru.\n",
    "\n",
    "validation_split=0.2 artinya kita tidak memberi semua data pelatihan untuk robot. Kita sisihkan 20% sebagai ujian yang tidak boleh dia lihat dulu. Ujian ini dilakukan supaya kita tahu seberapa baik dia bisa belajar dan menerapkan pengetahuannya pada data yang belum pernah dilihat.\n",
    "\n",
    "### 4. Parameter callbacks=[early_stopping]\n",
    "- Callbacks adalah parameter untuk menjalankan fungsi-fungsi khusus selama pelatihan. Di sini, early_stopping adalah fungsi callback yang berhenti jika kinerja model pada data validasi tidak membaik untuk beberapa epoch berturut-turut.\n",
    "- early_stopping akan menghentikan pelatihan ketika tidak ada perbaikan pada metrik validasi dalam 10 epoch terakhir, yang mencegah overfitting dan mempercepat pelatihan.\n",
    "\n",
    "callbacks=[early_stopping] artinya kita akan menghentikan pelatihan kalau robot tidak makin pintar lagi setelah beberapa kali belajar. Kalau selama 10 kali belajar, robot sudah tidak menunjukkan peningkatan, kita bilang, \"Stop, sudah cukup belajar.\"\n",
    "\n",
    "### 5. verbose=2\n",
    "- verbose mengatur tampilan output pelatihan. Dengan verbose=2, informasi pelatihan disajikan dengan ringkas, mencetak ringkasan setiap epoch tanpa tampilan batang kemajuan.\n",
    "\n",
    "verbose=2 adalah cara kita melihat bagaimana proses belajar robot. Di sini, kita memilih gaya ringkas, jadi kita melihat hasilnya saja setiap kali dia menyelesaikan satu putaran pembelajaran, tanpa menampilkan banyak detail kecil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d94c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(trainX, trainY, epochs=200, batch_size=32, validation_data=(testX, testY) , verbose=2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe41746",
   "metadata": {},
   "source": [
    "# 7. Hitung RMSE dan MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c556995",
   "metadata": {},
   "source": [
    "Hasil dari uji RMSE dan MAE adalah sebagai berikut:\n",
    "- \"< 10%\" dari rata-rata target: Dianggap sangat baik, menunjukkan bahwa model sangat akurat.\n",
    "- \"10%-20%\" dari rata-rata target: Masih bisa diterima tergantung pada konteks aplikasi.\n",
    "- \"> 20%\" dari rata-rata target: Umumnya dianggap kurang baik, namun tetap tergantung pada standar industri atau aplikasi spesifik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2276927",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScore_RMSE = np.sqrt(mean_squared_error(trainY[0], trainPredict[:, 0]))\n",
    "testScore_RMSE = np.sqrt(mean_squared_error(testY[0], testPredict[:, 0]))\n",
    "trainScore_MAE = mean_absolute_error(trainY[0], trainPredict[:, 0])\n",
    "testScore_MAE = mean_absolute_error(testY[0], testPredict[:, 0])\n",
    "\n",
    "print(f\"\"\"berikut adalah hasil trainScore_RMSE = {trainScore_RMSE}\"\"\")\n",
    "print(f\"\"\"berikut adalah hasil testScore_RMSE = {testScore_RMSE}\"\"\")\n",
    "print(f\"\"\"berikut adalah hasil trainScore_MAE = {trainScore_MAE}\"\"\")\n",
    "print(f\"\"\"berikut adalah hasil testScore_MAE = {testScore_MAE}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3db07",
   "metadata": {},
   "source": [
    "# 8. Buat grafik untuk visualisasi hasil berdasarkan train dan test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(latih.history['loss'], label='train_loss')\n",
    "plt.plot(latih.history['val_loss'], label='val_loss')\n",
    "plt.title('Model Loss over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f05dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(scaler.inverse_transform(dataset), label='Actual Data')\n",
    "plt.plot(trainPredictPlot, label='Train Predictions')\n",
    "plt.plot(testPredictPlot, label='Test Predictions')\n",
    "plt.title('Actual vs Predicted Airline Passengers')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e73508",
   "metadata": {},
   "source": [
    "# 9. Lakukan prediksi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c223816",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Menggunakan lebih banyak data historis untuk konteks awal\n",
    "look_back = 12  # Menyusun ulang untuk 12 bulan musim sebagai input\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# Bentuk input kembali ke model\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], look_back, 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], look_back, 1))\n",
    "\n",
    "\n",
    "# Memperbarui model untuk bentuk input yang baru\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(look_back, 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Melatih model kembali\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=32, validation_data=(testX, testY) , verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# Prediksi masa depan dengan input batch\n",
    "initial_input = testX[-1]  # Ambil data akhir dari test set sebagai konteks awal untuk prediksi\n",
    "future_predictions = []\n",
    "\n",
    "# Loop untuk 12 bulan ke depan\n",
    "current_input = initial_input.reshape(1, look_back, 1)\n",
    "for _ in range(12):\n",
    "    # Prediksi bulan berikutnya\n",
    "    next_pred = model.predict(current_input)\n",
    "    \n",
    "    # Simpan prediksi dan perbarui input\n",
    "    future_predictions.append(next_pred[0, 0])\n",
    "    \n",
    "    # Memperbarui input untuk prediksi berikutnya\n",
    "    current_input = np.roll(current_input, -1, axis=1)  # Shift data\n",
    "    current_input[0, -1, 0] = next_pred  # Tambahkan prediksi terbaru di akhir\n",
    "\n",
    "# Inversi normalisasi pada prediksi\n",
    "future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "future_predictions = scaler.inverse_transform(future_predictions)\n",
    "\n",
    "# Menampilkan hasil prediksi\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.plot(scaler.inverse_transform(dataset), label='Actual Data')\n",
    "plt.plot(np.arange(len(dataset), len(dataset) + 12), future_predictions, label='12-Month Forecast', color='red')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Number of Passengers')\n",
    "plt.title('Airline Passenger Predictions for Next 12 Months')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb056d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
